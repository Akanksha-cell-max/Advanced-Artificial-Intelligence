{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbgmAg1jB6IA6WDhJEQvYK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akanksha-cell-max/Advanced-Artificial-Intelligence/blob/main/Practical_No_5_Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cl57CLfizCiq",
        "outputId": "92bc2515-7a2e-46dd-b437-266e05150a89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed!\n",
            "\n",
            "Final Q-Table:\n",
            "\n",
            "[[[ 34.78478561  42.612659    35.43487018  31.805103  ]\n",
            "  [ -1.46727312  46.73468907   8.46630234   5.4479647 ]\n",
            "  [ -0.94370472  52.0360332   -1.02902615   8.64533862]\n",
            "  [  2.42722796  59.9104745    0.23693086  -0.63757766]\n",
            "  [ -0.57655314   2.17008318  -0.52176003  -0.58441256]]\n",
            "\n",
            " [[ 34.71903699  23.07066325  35.96046766  48.45851   ]\n",
            "  [ 25.85185918  37.09204022  34.57526347  54.9539    ]\n",
            "  [ 37.81589775  59.49984557  39.12059637  62.171     ]\n",
            "  [ 44.81870219  70.19        49.58701568  27.6697026 ]\n",
            "  [ -0.51821273  59.72304238   5.82145361   1.26199766]]\n",
            "\n",
            " [[  3.37613809  -1.0454856    0.17617728  42.06348334]\n",
            "  [  4.27393532   0.79389807  -0.64457877  60.60021567]\n",
            "  [  6.95412577  23.07069417  15.62620444  70.18498873]\n",
            "  [ 59.6666639   79.1         57.08585497  67.16499715]\n",
            "  [  5.73753319  87.80301141  32.61055787  24.87012429]]\n",
            "\n",
            " [[ -0.65133966  -0.61304608  -0.58519851   0.21822906]\n",
            "  [ -0.4019131   -0.4033485   -0.52774278  18.10356211]\n",
            "  [ -0.15346117  76.19704986  -0.32914432   7.739     ]\n",
            "  [ 59.6569393   89.          54.65160643  80.99460679]\n",
            "  [ 31.30130173  99.94360791  27.19067897  44.76931347]]\n",
            "\n",
            " [[ -0.41862912  -0.3940399   -0.3940399   -0.33632637]\n",
            "  [ -0.298468    -0.2881      -0.21601     26.15137128]\n",
            "  [  6.0716451   34.13889974   3.22552061  88.90607318]\n",
            "  [ 68.55127448  70.64957914  70.74459357 100.        ]\n",
            "  [  0.           0.           0.           0.        ]]]\n",
            "\n",
            "Learned path from Start to Goal:\n",
            "(0, 0)\n",
            "(1, 0)\n",
            "(1, 1)\n",
            "(1, 2)\n",
            "(1, 3)\n",
            "(2, 3)\n",
            "(3, 3)\n",
            "(4, 3)\n",
            "(4, 4)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Define environment\n",
        "grid_size = 5\n",
        "goal_state = (4, 4)\n",
        "\n",
        "# Define actions\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "\n",
        "# Initialize Q-table\n",
        "q_table = np.zeros((grid_size, grid_size, len(actions)))\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1       # learning rate\n",
        "gamma = 0.9       # discount factor\n",
        "epsilon = 0.2     # exploration rate\n",
        "episodes = 500\n",
        "\n",
        "# Helper function to get next state\n",
        "def get_next_state(state, action):\n",
        "    i, j = state\n",
        "    if action == 'up' and i > 0:\n",
        "        i -= 1\n",
        "    elif action == 'down' and i < grid_size - 1:\n",
        "        i += 1\n",
        "    elif action == 'left' and j > 0:\n",
        "        j -= 1\n",
        "    elif action == 'right' and j < grid_size - 1:\n",
        "        j += 1\n",
        "    return (i, j)\n",
        "\n",
        "# Helper function for reward\n",
        "def get_reward(state):\n",
        "    if state == goal_state:\n",
        "        return 100\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "# Training\n",
        "for episode in range(episodes):\n",
        "    state = (0, 0)  # start state\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        if random.uniform(0,1) < epsilon:\n",
        "            action_idx = random.randint(0, len(actions)-1)  # explore\n",
        "        else:\n",
        "            action_idx = np.argmax(q_table[state[0], state[1]])  # exploit\n",
        "\n",
        "        action = actions[action_idx]\n",
        "        next_state = get_next_state(state, action)\n",
        "        reward = get_reward(next_state)\n",
        "\n",
        "        # Update Q-value\n",
        "        old_value = q_table[state[0], state[1], action_idx]\n",
        "        next_max = np.max(q_table[next_state[0], next_state[1]])\n",
        "\n",
        "        new_value = old_value + alpha * (reward + gamma * next_max - old_value)\n",
        "        q_table[state[0], state[1], action_idx] = new_value\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if state == goal_state:\n",
        "            done = True\n",
        "\n",
        "print(\"Training completed!\\n\")\n",
        "print(\"Final Q-Table:\\n\")\n",
        "print(q_table)\n",
        "\n",
        "# Test learned policy\n",
        "def test_policy():\n",
        "    state = (0, 0)\n",
        "    path = [state]\n",
        "    steps = 0\n",
        "    while state != goal_state and steps < 50:\n",
        "        action_idx = np.argmax(q_table[state[0], state[1]])\n",
        "        action = actions[action_idx]\n",
        "        state = get_next_state(state, action)\n",
        "        path.append(state)\n",
        "        steps += 1\n",
        "    return path\n",
        "\n",
        "path = test_policy()\n",
        "\n",
        "print(\"\\nLearned path from Start to Goal:\")\n",
        "for step in path:\n",
        "    print(step)\n"
      ]
    }
  ]
}